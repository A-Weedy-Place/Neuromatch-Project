# -*- coding: utf-8 -*-
"""HCP_2020_fc_categorical

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14wjFzDzHfd4myjljKwk0QbmAbXnM7wBI

# Human Connectome Project (HCP) Dataset loader

The HCP dataset comprises resting-state and task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest.
"""

# @title Install dependencies
!pip install nilearn --quiet

import os
import numpy as np
import matplotlib.pyplot as plt

# Necessary for visualization
from nilearn import plotting, datasets

# Commented out IPython magic to ensure Python compatibility.
#@title Figure settings
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
plt.style.use("https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle")

"""# Basic parameters"""

# The download cells will store the data in nested directories starting here:
HCP_DIR = "./DATA"
if not os.path.isdir(HCP_DIR):
  os.mkdir(HCP_DIR)

# The data shared for NMA projects is a subset of the full HCP dataset
N_SUBJECTS = 338

# The data have already been aggregated into ROIs from the Glasesr parcellation
N_PARCELS = 360

# The acquisition parameters for all tasks were identical
TR = 0.72  # Time resolution, in sec

# The parcels are matched across hemispheres with the same order
HEMIS = ["Right", "Left"]

# Each experiment was repeated multiple times in each subject
N_RUNS_REST = 4
N_RUNS_TASK = 2

# Time series data are organized by experiment, with each experiment
# having an LR and RL (phase-encode direction) acquistion
BOLD_NAMES = [
  "rfMRI_REST1_LR", "rfMRI_REST1_RL",
  "rfMRI_REST2_LR", "rfMRI_REST2_RL",
  "tfMRI_MOTOR_RL", "tfMRI_MOTOR_LR",
  "tfMRI_WM_RL", "tfMRI_WM_LR",
  "tfMRI_EMOTION_RL", "tfMRI_EMOTION_LR",
  "tfMRI_GAMBLING_RL", "tfMRI_GAMBLING_LR",
  "tfMRI_LANGUAGE_RL", "tfMRI_LANGUAGE_LR",
  "tfMRI_RELATIONAL_RL", "tfMRI_RELATIONAL_LR",
  "tfMRI_SOCIAL_RL", "tfMRI_SOCIAL_LR"
]

# You may want to limit the subjects used during code development.
# This will use all subjects:
subjects = range(N_SUBJECTS)

"""# Downloading data

The rest and task data are shared in different files, but they will unpack into the same directory structure.

Each file is fairly large and will take some time to download. If you are focusing only on rest or task analyses, you may not want to download only that dataset.

We also separately provide some potentially useful behavioral covariate information.
"""

# @title Download the data

# @markdown Task data in `HCP_DIR/hcp_task`, rest in `HCP_DIR/hcp_rest`, covariate in `HCP_DIR/hcp`

import os, requests, tarfile

fnames = ["hcp_rest.tgz",
          "hcp_task.tgz",
          "hcp_covariates.tgz",
          "atlas.npz"]
urls = ["https://osf.io/bqp7m/download",
        "https://osf.io/s4h8j/download",
        "https://osf.io/x5p4g/download",
        "https://osf.io/j5kuc/download"]

for fname, url in zip(fnames, urls):
  if not os.path.isfile(fname):
    try:
      r = requests.get(url)
    except requests.ConnectionError:
      print("!!! Failed to download data !!!")
    else:
      if r.status_code != requests.codes.ok:
        print("!!! Failed to download data !!!")
      else:
        print(f"Downloading {fname}...")
        with open(fname, "wb") as fid:
          fid.write(r.content)
        print(f"Download {fname} completed!")

# @title Extract the data in `HCP_DIR`
fnames = ["hcp_covariates", "hcp_rest", "hcp_task"]

for fname in fnames:
  # open file
  path_name = os.path.join(HCP_DIR, fname)
  if not os.path.exists(path_name):
    print(f"Extracting {fname}.tgz...")
    with tarfile.open(f"{fname}.tgz") as fzip:
      fzip.extractall(HCP_DIR)
  else:
    print(f"File {fname}.tgz has already been extracted.")

"""## Loading region information

Downloading either dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.

Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).

Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/).
"""

dir = os.path.join(HCP_DIR, "hcp_task")  # choose the data directory
regions = np.load(os.path.join(dir, "regions.npy")).T
region_info = dict(name=regions[0].tolist(),
                   network=regions[1],
                   myelin=regions[2].astype(float)
                   )

"""We also provide the [parcellation on the fsaverage5 surface](https://figshare.com/articles/HCP-MMP1_0_projected_on_fsaverage/3498446) and approximate MNI coordinates of each region, which can be useful for visualization:"""

with np.load(f"atlas.npz") as dobj:
  atlas = dict(**dobj)

"""# Helper functions

## Data loading
"""

def get_image_ids(name):
  """Get the 1-based image indices for runs in a given experiment.

    Args:
      name (str) : Name of experiment ("rest" or name of task) to load
    Returns:
      run_ids (list of int) : Numeric ID for experiment image files

  """
  run_ids = [
             i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code
             ]
  if not run_ids:
    raise ValueError(f"Found no data for '{name}'")
  return run_ids


def load_timeseries(subject, name, dir,
                    runs=None, concat=True, remove_mean=True):
  """Load timeseries data for a single subject.

  Args:
    subject (int): 0-based subject ID to load
    name (str) : Name of experiment ("rest" or name of task) to load
    dir (str) : data directory
    run (None or int or list of ints): 0-based run(s) of the task to load,
      or None to load all runs.
    concat (bool) : If True, concatenate multiple runs in time
    remove_mean (bool) : If True, subtract the parcel-wise mean

  Returns
    ts (n_parcel x n_tp array): Array of BOLD data values

  """
  # Get the list relative 0-based index of runs to use
  if runs is None:
    runs = range(N_RUNS_REST) if name == "rest" else range(N_RUNS_TASK)
  elif isinstance(runs, int):
    runs = [runs]

  # Get the first (1-based) run id for this experiment
  offset = get_image_ids(name)[0]

  # Load each run's data
  bold_data = [
               load_single_timeseries(subject,
                                      offset + run,
                                      dir,
                                      remove_mean) for run in runs
               ]

  # Optionally concatenate in time
  if concat:
    bold_data = np.concatenate(bold_data, axis=-1)

  return bold_data


def load_single_timeseries(subject, bold_run, dir, remove_mean=True):
  """Load timeseries data for a single subject and single run.

  Args:
    subject (int): 0-based subject ID to load
    bold_run (int): 1-based run index, across all tasks
    dir (str) : data directory
    remove_mean (bool): If True, subtract the parcel-wise mean

  Returns
    ts (n_parcel x n_timepoint array): Array of BOLD data values

  """
  bold_path = os.path.join(dir, "subjects", str(subject), "timeseries")
  bold_file = f"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy"
  ts = np.load(os.path.join(bold_path, bold_file))
  if remove_mean:
    ts -= ts.mean(axis=1, keepdims=True)
  return ts


def load_evs(subject, name, condition, dir):
  """Load EV (explanatory variable) data for one task condition.

  Args:
    subject (int): 0-based subject ID to load
    name (str) : Name of task
    condition (str) : Name of condition
    dir (str) : data directory

  Returns
    evs (list of dicts): A dictionary with the onset, duration, and amplitude
      of the condition for each run.

  """
  evs = []
  for id in get_image_ids(name):
    task_key = BOLD_NAMES[id - 1]
    ev_file = os.path.join(dir, "subjects", str(subject), "EVs",
                           task_key, f"{condition}.txt")
    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)
    ev = dict(zip(["onset", "duration", "amplitude"], ev_array))
    evs.append(ev)
  return evs

def load_evs_v2(subject, name, condition, dir):
  """Load EV (explanatory variable) data for one task condition.

  Args:
    subject (int): 0-based subject ID to load
    name (str) : Name of task
    condition (str) : Name of condition
    dir (str) : data directory

  Returns
    evs (list of dicts): A dictionary with the onset, duration, and amplitude
      of the condition for each run.

  """
  evs = []
  for id in get_image_ids(name):
    task_key = BOLD_NAMES[id - 1]
    ev_file = os.path.join(dir, "subjects", str(subject), "EVs",
                           task_key, f"{condition}.txt")
    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)
    ev = dict(zip(["onset", "duration"], ev_array))
    evs.append(ev)
  return evs

"""## Task-based analysis"""

def condition_frames_v2(run_evs, num_measurements=5, skip=0):
    """Identify timepoints corresponding to a given condition in each run.

    Args:
        run_evs (list of dicts): Onset and duration of the event, per run
        num_measurements (int): Number of measurements to take after each onset
        skip (int): Ignore this many frames at the start of each trial, to account
            for hemodynamic lag

    Returns:
        frames_list (list of 1D arrays): Flat arrays of frame indices, per run
    """
    frames_list = []
    for ev in run_evs:
        # Determine when trial starts, rounded down
        start = np.floor(ev["onset"] / TR).astype(int)

        # Take the next num_measurements frames after each onset
        frames = [s + np.arange(skip, skip + num_measurements) for s in start]

        frames_list.append(frames)

    return frames_list

def selective_average_v2(timeseries_data, ev, num_measurements=5, skip=0):
    """Take the temporal mean across frames for a given condition.

    Args:
        timeseries_data (array or list of arrays): n_parcel x n_tp arrays
        ev (dict or list of dicts): Condition timing information
        num_measurements (int): Number of measurements to take after each onset
        skip (int): Ignore this many frames at the start of each trial, to account
            for hemodynamic lag

    Returns:
        avg_data_list (list of 2D arrays): List of arrays with averaged data per onset
    """
    # Ensure that we have lists of the same length
    if not isinstance(timeseries_data, list):
        timeseries_data = [timeseries_data]
    if not isinstance(ev, list):
        ev = [ev]
    if len(timeseries_data) != len(ev):
        raise ValueError("Length of `timeseries_data` and `ev` must match.")

    # Identify the indices of relevant frames
    frames = condition_frames_v2(ev, num_measurements, skip)

    avg_data_list = []

    # Select the frames from each image
    for run_data, run_frames in zip(timeseries_data, frames):
        run_avg_data = []
        for trial_frames in run_frames:
            trial_frames = trial_frames[trial_frames < run_data.shape[1]]
            avg_data = run_data[:, trial_frames].mean(axis=1)  # Average across the frames
            run_avg_data.append(avg_data)
        avg_data_list.append(np.array(run_avg_data))

    return avg_data_list

timeseries_task = []
for subject in subjects:
  timeseries_task.append(load_timeseries(subject, "gambling",
                                         dir=os.path.join(HCP_DIR, "hcp_task"),
                                         concat=False))

"""## Load individual runs for a given task

Load each subject's data for a specific task, separately for each run:

## Run a simple subtraction analysis
"""

task = "gambling"
conditions = ["loss_event", "win_event"]  # Run a substraction analysis between two conditions

win = []
loss = []

for subject in subjects:

  # Get the average signal in each region for each condition
  loss_evs = load_evs_v2(subject, task, "loss_event", dir=os.path.join(HCP_DIR, "hcp_task"))
  loss_avgs = selective_average_v2(timeseries_task[subject], loss_evs)

  win_evs = load_evs_v2(subject, task, "win_event", dir=os.path.join(HCP_DIR, "hcp_task"))
  win_avgs = selective_average_v2(timeseries_task[subject], win_evs)

  all_loss = np.concatenate(loss_avgs, axis=0)
  all_win = np.concatenate(win_avgs, axis=0)

  loss.append(all_loss)
  win.append(all_win)



from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import LeaveOneGroupOut, cross_validate
from sklearn.metrics import roc_auc_score
import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category = ConvergenceWarning)

mean_acc_scores_all = []
std_acc_scores_all = []
feature_weights_all = []
Y = np.concatenate((np.zeros((29, 1)), np.ones((29, 1))), axis=0)
y = Y.squeeze()
runs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]

for subject in subjects:

  X = np.vstack((loss[subject], win[subject]))

  # Create the output array with the specified outputs

  lsvc = LinearSVC()

  logo = LeaveOneGroupOut()
  logo.split(X, y, runs)

  scores_by_run = cross_validate(lsvc, X, y, cv = logo, groups = runs)

  mean_score = scores_by_run['test_score'].mean()
  std_score = scores_by_run['test_score'].std()

  mean_acc_scores_all.append(mean_score)
  std_acc_scores_all.append(std_score)

  lsvc.fit(X,y)
  feature_weights = lsvc.coef_[0]
  feature_weights_list = list(enumerate(feature_weights))
  feature_names = region_info["name"]

  feature_weights_all.append(feature_weights_list)

len(mean_acc_scores_all)

plt.hist(mean_acc_scores_all)

import numpy as np

# Assuming feature_weights_all is a list of 214 elements, each containing a list of lists or numpy array of shape (360, 2)
# Each array has two columns: first column is index numbers, second column is feature weights

def average_feature_weights(feature_weights_all):
    num_subjects = len(feature_weights_all)

    # Initialize an array to hold the sum of weights for each feature index
    total_weights = np.zeros(360)

    # Iterate over each subject
    for subject in feature_weights_all:
        # Convert subject data to numpy array if it's not already
        subject_array = np.array(subject)

        # Extract the feature weights (second column)
        weights = subject_array[:, 1]

        # Accumulate the weights
        total_weights += weights

    # Calculate the average weights
    average_weights = total_weights / num_subjects

    return average_weights

# Example usage
# feature_weights_all = [subject_1_data, subject_2_data, ..., subject_214_data]
# Each subject_x_data is a list of lists or numpy array of shape (360, 2)

# Calculate the average feature weights
average_weights = average_feature_weights(feature_weights_all)

# Create a list of tuples containing feature indices and corresponding weights
feature_weights_list = list(enumerate(average_weights))
feature_name_list = [feature_weights_list[i] + (feature_names[i],) for i in range(len(feature_weights_list))]
# Sort the feature weights in descending order of absolute values
sorted_feature_weights = sorted(feature_name_list, key=lambda x: x[1], reverse = True)
top_10_features = sorted_feature_weights[:10]

top_feature_indices = []
top_feature_names = []
top_feature_weights = []
for i in range(len(top_10_features)):
    top_feature_indices.append(top_10_features[i][0])
    top_feature_weights.append(top_10_features[i][1])
    top_feature_names.append(top_10_features[i][2])

top_feature_names

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.bar(range(len(top_feature_names)), top_feature_weights, align='center')
plt.xticks(range(len(top_feature_names)), top_feature_names)
plt.xlabel('Feature Index')
plt.ylabel('Feature Weight')
plt.title('Top 10 Most Important Features for SVM')
plt.show()

import numpy as np
from nilearn import plotting, datasets

# Assuming 'average_weights' is the array of average feature weights
# Assuming 'top_feature_indices' is the list of indices of the top 10 features

# Create a mask for the top 10 features
top_10_mask = np.zeros_like(average_weights)
for index in top_feature_indices:
    top_10_mask[index] = average_weights[index]

# Fetch fsaverage dataset for surface plotting
fsaverage = datasets.fetch_surf_fsaverage()

# Visualize the left hemisphere
surf_contrast = top_10_mask[atlas["labels_L"]]
plotting.view_surf(fsaverage['infl_left'],
                          surf_contrast,
                          vmax=.0003,
                          title='Top 10 Feature Weights (Left Hemisphere)')

# If you also want to visualize the right hemisphere, repeat for the right side
surf_contrast_right = top_10_mask[atlas["labels_R"]]
plotting.view_surf(fsaverage['infl_right'],
                                surf_contrast_right,
                                vmax=.0003,
                                title='Top 10 Feature Weights (Right Hemisphere)')

# This uses the nilearn package
from nilearn import plotting, datasets

# Try both hemispheres (L->R and left->right)
fsaverage = datasets.fetch_surf_fsaverage()
surf_contrast = average_weights[atlas["labels_L"]]
plotting.view_surf(fsaverage['infl_left'],
                   surf_contrast,
                   vmax=.0003)

# This uses the nilearn package
from nilearn import plotting, datasets

# Try both hemispheres (L->R and left->right)
fsaverage = datasets.fetch_surf_fsaverage()
surf_contrast = average_weights[atlas["labels_L"]]
plotting.view_surf(fsaverage['infl_left'],
                   surf_contrast,
                   vmax=.0003)

fsaverage = datasets.fetch_surf_fsaverage()
surf_contrast = average_weights[atlas["labels_R"]]
plotting.view_surf(fsaverage['infl_right'],
                   surf_contrast,
                   vmax=.0003)

# from sklearn.ensemble import RandomForestClassifier

# forest_mean_acc_scores_all = []
# forest_std_acc_scores_all = []
# forest_feature_weights_all = []
# Y = np.concatenate((np.zeros((29, 1)), np.ones((29, 1))), axis=0)
# y = Y.squeeze()
# runs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]

# for subject in subjects:

#   X = np.vstack((loss[subject], win[subject]))

#   # Create the output array with the specified outputs

#   forest = RandomForestClassifier()

#   logo = LeaveOneGroupOut()
#   logo.split(X, y, runs)

#   scores_by_run = cross_validate(forest, X, y, cv = logo, groups = runs)

#   forest_mean_score = scores_by_run['test_score'].mean()
#   forest_std_score = scores_by_run['test_score'].std()

#   forest_mean_acc_scores_all.append(forest_mean_score)
#   forest_std_acc_scores_all.append(forest_std_score)

#   forest.fit(X,y)
#   forest_feature_weights = lsvc.coef_[0]
#   forest_feature_weights_list = list(enumerate(feature_weights))
#   feature_names = region_info["name"]

#   forest_feature_weights_all.append(feature_weights_list)

# plt.hist(mean_acc_scores_all, alpha = 0.5 )
# plt.hist(forest_mean_acc_scores_all, alpha = 0.5)

# absolute_difference_list = []
# for i in range(214):
#     absolute_difference = abs(mean_acc_scores_all[i] - forest_mean_acc_scores_all[i])
#     absolute_difference_list.append(absolute_difference)

# # Print the resulting list of absolute differences
# print("List of absolute differences:", absolute_difference_list)

# # Create a histogram of the absolute differences
# plt.hist(absolute_difference_list, edgecolor='black')
# plt.title('Histogram of Absolute Differences')
# plt.xlabel('Absolute Difference')
# plt.ylabel('Frequency')
# plt.show()

timeseries_rest = []
for subject in subjects:
  ts_concat = load_timeseries(subject, name="rest",
                              dir=os.path.join(HCP_DIR, "hcp_rest"))
  timeseries_rest.append(ts_concat)

# Initialize a new 3D list to store the averaged values
averaged_data = []

# Compute the average for each feature for each subject
for subject in range(len(timeseries_rest)):
    subject_data = []
    for feature in range(len(timeseries_rest[subject])):
        average_value = np.mean(timeseries_rest[subject][feature])
        subject_data.append([average_value])
    averaged_data.append(subject_data)

# Convert the list to a NumPy array for easy manipulation and printing
averaged_data = np.array(averaged_data)

averaged_data.shape

len(mean_acc_scores_all)

len(region_info["name"])

import numpy as np
from scipy.stats import pearsonr

# Assuming 'averaged_data', 'mean_acc_scores_all', and 'region_info' are already defined
# averaged_data: np.ndarray of shape (338, 360, 1)
# mean_acc_scores_all: list of length 338
# region_info: dictionary with key "name" having a list of 360 region names

# Initialize a list to store correlation coefficients
correlation_coefficients = []

# Flatten the third dimension of averaged_data for easier access
flattened_data = averaged_data.reshape(338, 360)

# Calculate correlation for each region
for region_idx in range(360):
    region_data = flattened_data[:, region_idx]
    corr, _ = pearsonr(region_data, mean_acc_scores_all)
    correlation_coefficients.append(corr)

# Convert to a numpy array for easier manipulation
correlation_coefficients = np.array(correlation_coefficients)

# Find the indices of the top 10 correlations
top_20_indices = np.argsort(np.abs(correlation_coefficients))[-20:][::-1]

# Print the top 10 brain regions and their correlation coefficients
print("Top 20 Brain Regions by Correlation Coefficient:")
for idx in top_20_indices:
    print(f"Region: {region_info['name'][idx]}, Correlation Coefficient: {correlation_coefficients[idx]}")

# # This uses the nilearn package
# from nilearn import plotting, datasets

# # Try both hemispheres (L->R and left->right)
# fsaverage = datasets.fetch_surf_fsaverage()
# surf_contrast = correlation_coefficients[atlas["labels_L"]]
# plotting.view_surf(fsaverage['infl_left'],
#                    surf_contrast,
#                    vmax=.18)

# fsaverage = datasets.fetch_surf_fsaverage()
# surf_contrast = correlation_coefficients[atlas["labels_R"]]
# plotting.view_surf(fsaverage['infl_right'],
#                    surf_contrast,
#                    vmax=.15)

def condition_frames(run_evs, skip=0):
  """Identify timepoints corresponding to a given condition in each run.

  Args:
    run_evs (list of dicts) : Onset and duration of the event, per run
    skip (int) : Ignore this many frames at the start of each trial, to account
      for hemodynamic lag

  Returns:
    frames_list (list of 1D arrays): Flat arrays of frame indices, per run

  """
  frames_list = []
  for ev in run_evs:

    # Determine when trial starts, rounded down
    start = np.floor(ev["onset"] / TR).astype(int)

    # Use trial duration to determine how many frames to include for trial
    duration = np.ceil(ev["duration"] / TR).astype(int)

    # Take the range of frames that correspond to this specific trial
    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]

    frames_list.append(np.concatenate(frames))

  return frames_list


def selective_average(timeseries_data, ev, skip=0):
  """Take the temporal mean across frames for a given condition.

  Args:
    timeseries_data (array or list of arrays): n_parcel x n_tp arrays
    ev (dict or list of dicts): Condition timing information
    skip (int) : Ignore this many frames at the start of each trial, to account
      for hemodynamic lag

  Returns:
    avg_data (1D array): Data averagted across selected image frames based
    on condition timing

  """
  # Ensure that we have lists of the same length
  if not isinstance(timeseries_data, list):
    timeseries_data = [timeseries_data]
  if not isinstance(ev, list):
    ev = [ev]
  if len(timeseries_data) != len(ev):
    raise ValueError("Length of `timeseries_data` and `ev` must match.")

  # Identify the indices of relevant frames
  frames = condition_frames(ev, skip)

  # Select the frames from each image
  selected_data = []
  for run_data, run_frames in zip(timeseries_data, frames):
    run_frames = run_frames[run_frames < run_data.shape[1]]
    selected_data.append(run_data[:, run_frames])

  # Take the average in each parcel
  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)

  return avg_data

task = "gambling"
conditions = ["win", "loss"]  # Run a substraction analysis between two conditions

contrast = []
for subject in subjects:

  # Get the average signal in each region for each condition
  evs = [load_evs(subject, task, cond, dir=os.path.join(HCP_DIR, "hcp_task")) for cond in conditions]
  avgs = [selective_average(timeseries_task[subject], ev) for ev in evs]

  # Store the region-wise difference
  contrast.append(avgs[0] - avgs[1])

group_contrast = np.mean(contrast, axis=0)

X = averaged_data.reshape(338,360)

import numpy as np
from sklearn.model_selection import cross_validate
from sklearn.linear_model import Lasso

# Assuming X, contrasts, top_feature_indices, and top_feature_names are already defined
# Example initialization (remove these if variables are already defined in your context)
# X = np.random.rand(338, 360)
# contrasts = np.random.rand(338, 360)
# top_feature_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # Example indices
# top_feature_names = ['Region1', 'Region2', 'Region3', 'Region4', 'Region5', 'Region6', 'Region7', 'Region8', 'Region9', 'Region10']  # Example names
contrast = np.array(contrast)

scores = []

for index, region_index in enumerate(top_feature_indices):
    y = contrast[:, region_index]

    lasso = Lasso()
    cv_results = cross_validate(lasso, X, y, cv=5)
    mean_score_lasso = cv_results['test_score'].mean()

    scores.append([mean_score_lasso, top_feature_names[index]])

# Convert scores list to a numpy array
scores_array = np.array(scores, dtype=object)

# Print or store the results
print(scores_array)

import pandas as pd

# Initial array
data = [
    [-0.0246117166563772, 'L_47m'],
    [-0.009170057598188475, 'R_13l'],
    [-0.032237697659648255, 'R_AVI'],
    [-0.026463268834170918, 'R_11l'],
    [-0.02279527258145895, 'L_11l'],
    [-0.008900166891367745, 'L_p47r'],
    [-0.014134561829696368, 'L_10v'],
    [-0.007476789622793456, 'L_13l'],
    [-0.019054047009575203, 'L_a47r'],
    [-0.021295335791243142, 'R_pOFC'],
]

# Convert values to four decimal points and extract names
formatted_data = [(name, f"{value:.4f}") for value, name in data]

# Create DataFrame
df = pd.DataFrame(formatted_data, columns=["ROI", "R^2"])

# Set the table title
table_title = "Lasso Linear Model Outcome (R^2) for Predicting Win vs. Loss BOLD Contrasts in Top Ten Regions of Feature Importance for Part 1's SVM Model"

# Display the DataFrame
print(f"\n{table_title}\n")
print(df.to_string(index=False))

y = contrast[:, region_index]

    lasso = Lasso()
    cv_results = cross_validate(lasso, X, y, cv=5)
    mean_score_lasso = cv_results['test_score'].mean()

    scores.append([mean_score_lasso, top_feature_names[index]])

# Convert scores list to a numpy array
scores_array = np.array(scores, dtype=object)

# Print or store the results
print(scores_array)



# fc = np.zeros((N_SUBJECTS, N_PARCELS, N_PARCELS))
# for sub, ts in enumerate(timeseries_rest):
#   fc[sub] = np.corrcoef(ts)

# group_fc = fc.mean(axis=0)

# plt.figure()
# plt.imshow(group_fc, interpolation="none", cmap="bwr", vmin=-1, vmax=1)
# plt.colorbar()
# plt.show()

# # The number of rows in the result array is N_SUBJECTS * (N_PARCELS * (N_PARCELS - 1)) / 2
# n_connections = N_PARCELS * (N_PARCELS) // 2
# result = []

# for subject in range(N_SUBJECTS):
#     subject_data = []
#     for i in range(N_PARCELS):
#         for j in range(i, N_PARCELS):
#             fc_value = fc[subject, i, j]
#             label = f"{region_info['name'][i]} X {region_info['name'][j]}"
#             subject_data.append([fc_value, label])
#     result.append(subject_data)

# result = np.array(result)

# print(result.shape)

# result[0][:15]

# fc_values = result[:, :, 0].astype(float)  # Shape: (N_SUBJECTS, N_CONNECTIONS)

# correlations_all = []
# sorted_indices = np.argsort(correlations)



# for fc in range(64980):
#   correlations = np.corrcoef(fc_values[:, fc], mean_acc_scores_all, rowvar=False)[-1, :-1]
#   correlations_all.append(correlations[0])

# sorted_indices = np.argsort(correlations_all)

# # Top 10 positively correlated FC features
# top_10_positive_indices = sorted_indices[-10:]
# top_10_positive_labels = result[0, top_10_positive_indices, 1]
# top_10_positive_values = []

# for indices in top_10_positive_indices:
#   top_10_positive_values.append(correlations_all[indices])

# # # Top 10 negatively correlated FC features
# top_10_negative_indices = sorted_indices[:10]
# top_10_negative_labels = result[0, top_10_negative_indices, 1]
# top_10_negative_values = []

# for indices in top_10_negative_indices:
#   top_10_negative_values.append(correlations_all[indices])


# # Print the results
# print("Top 10 positively correlated FC features:")
# for label, value in zip(top_10_positive_labels, top_10_positive_values):
#     print(f"{label}: {value}")

# print("\nTop 10 negatively correlated FC features:")
# for label, value in zip(top_10_negative_labels, top_10_negative_values):
#     print(f"{label}: {value}")

# X = fc_values
# y = mean_acc_scores_all

# X.shape

# y_cate = np.zeros(len(y), dtype=int)

# # Apply conditions
# y_cate[y > 0.68] = 1
# y_cate[(y <= 0.68) & (y >= 0.54)] = 2
# y_cate[y < 0.54] = 0

# # Convert y_cate to a list (if needed)
# y_cate = y_cate.tolist()

# # Print the results
# print("y_cate:", y_cate)

# from sklearn import linear_model
# lasso = linear_model.Lasso()

# cv_results = cross_validate(lasso, X, y, cv=5)

# mean_score_2nd_lasso = cv_results['test_score'].mean()

# from sklearn.ensemble import RandomForestRegressor

# # forest_mean_acc_scores_all = []


# # rf_fc = RandomForestRegressor()

# # rf_fc_cv_results = cross_validate(rf_fc, X, y, cv = 5)

# # mean_score_rf_fc = cv_results['test_score'].mean()

# # mean_score_rf_fca

# import pandas as pd
# import seaborn as sns


# df = pd.DataFrame({'contrast' : group_contrast,
#                    'roi' : region_info['name'],
#                    })
# # we will plot the left foot minus right foot contrast so we only need one plot


# df_sorted = df.sort_values(by='contrast', ascending=False)

# # Selecting top 10 and bottom 10 regions
# top_10 = df_sorted.head(10)
# bottom_10 = df_sorted.tail(10)

# # Plotting top 10 regions
# plt.figure(figsize=(10, 6))
# sns.barplot(y='roi', x='contrast', data=top_10, palette='viridis')
# plt.title('Top 10 Regions with Highest Contrast')
# plt.xlabel('Contrast')
# plt.ylabel('Region of Interest')
# plt.show()

# # Plotting bottom 10 regions
# plt.figure(figsize=(10, 6))
# sns.barplot(y='roi', x='contrast', data=bottom_10, palette='viridis')
# plt.title('Top 10 Regions with Lowest Contrast')
# plt.xlabel('Contrast')
# plt.ylabel('Region of Interest')
# plt.show()

